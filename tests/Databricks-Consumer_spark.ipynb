{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37eb269b-a215-4340-9e97-67b2434d3b78",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Comprobar los puertos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34573736-20a6-4c2b-86ab-f7ac4690efa0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to kafka-azure.norwayeast.cloudapp.azure.com:29090 failed: timed out\nFalse\nTrue\nTrue\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "def check_port(host, port):\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    s.settimeout(5)\n",
    "    try:\n",
    "        s.connect((host, port))\n",
    "        return True\n",
    "    except socket.error as e:\n",
    "        print(f\"Connection to {host}:{port} failed: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        s.close()\n",
    "\n",
    "print(check_port(\"kafka-azure.norwayeast.cloudapp.azure.com\", 29090))\n",
    "print(check_port(\"kafka-azure.norwayeast.cloudapp.azure.com\", 9091))\n",
    "print(check_port(\"kafka-azure.norwayeast.cloudapp.azure.com\", 9090))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c99edf2-645c-4dcb-805d-d6c3cc3fa3cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f08a3980-bad5-4035-8f49-d788574f7110",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting kafka\n  Downloading kafka-1.3.5-py2.py3-none-any.whl (207 kB)\nInstalling collected packages: kafka\nSuccessfully installed kafka-1.3.5\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting fastavro\n  Downloading fastavro-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\nInstalling collected packages: fastavro\nSuccessfully installed fastavro-1.9.4\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nRequirement already satisfied: requests in /databricks/python3/lib/python3.9/site-packages (2.27.1)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests) (2.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests) (1.26.9)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests) (2021.10.8)\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting influxdb-client\n  Downloading influxdb_client-1.43.0-py3-none-any.whl (744 kB)\nRequirement already satisfied: python-dateutil>=2.5.3 in /databricks/python3/lib/python3.9/site-packages (from influxdb-client) (2.8.2)\nRequirement already satisfied: setuptools>=21.0.0 in /databricks/python3/lib/python3.9/site-packages (from influxdb-client) (61.2.0)\nRequirement already satisfied: certifi>=14.05.14 in /databricks/python3/lib/python3.9/site-packages (from influxdb-client) (2021.10.8)\nRequirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.9/site-packages (from influxdb-client) (1.26.9)\nCollecting reactivex>=4.0.4\n  Downloading reactivex-4.0.4-py3-none-any.whl (217 kB)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil>=2.5.3->influxdb-client) (1.16.0)\nRequirement already satisfied: typing-extensions<5.0.0,>=4.1.1 in /databricks/python3/lib/python3.9/site-packages (from reactivex>=4.0.4->influxdb-client) (4.1.1)\nInstalling collected packages: reactivex, influxdb-client\nSuccessfully installed influxdb-client-1.43.0 reactivex-4.0.4\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install kafka\n",
    "%pip install fastavro\n",
    "%pip install requests\n",
    "%pip install influxdb-client\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6076b42b-82db-49f3-b3fd-b48e960eac8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting kafka-python\n  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\nInstalling collected packages: kafka-python\nSuccessfully installed kafka-python-2.0.2\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade kafka-python\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b163422-cb0a-4bca-bf9b-df3d1e9c9bbb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.streaming import DataStreamWriter\n",
    "from kafka import KafkaConsumer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, substring, expr\n",
    "from pyspark.sql.avro.functions import from_avro\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "import requests\n",
    "import json\n",
    "from influxdb_client import InfluxDBClient, Point, WritePrecision\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "import datetime as dt\n",
    "from io import BytesIO\n",
    "import fastavro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2a0757b-a940-4c1b-be00-2583859db637",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Codigo del consumidor databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a12df82c-9a84-4416-b516-d080290f1fc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\":\"record\",\"name\":\"CryptoCurrency\",\"fields\":[{\"name\":\"nombre\",\"type\":\"string\"},{\"name\":\"bid\",\"type\":\"double\"},{\"name\":\"bid_size\",\"type\":\"double\"},{\"name\":\"ask\",\"type\":\"double\"},{\"name\":\"ask_size\",\"type\":\"double\"},{\"name\":\"daily_change\",\"type\":\"double\"},{\"name\":\"daily_change_percentage\",\"type\":\"double\"},{\"name\":\"precio_ultimo\",\"type\":\"double\"},{\"name\":\"volume\",\"type\":\"double\"},{\"name\":\"precio_maximo\",\"type\":\"double\"},{\"name\":\"precio_minimo\",\"type\":\"double\"}]}\n"
     ]
    }
   ],
   "source": [
    "schema_url = requests.get('{}/subjects/{}/versions/latest/schema'.format(\"http://kafka-azure.norwayeast.cloudapp.azure.com:8081\", \"criptomonedas\"))\n",
    "schema = schema_url.text\n",
    "\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8a0d683-50fd-40a4-8b17-707e21964a3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#:::::::::::CONFIGURACION::::::::::::\n",
    "kafka_brokers = \"kafka-azure.norwayeast.cloudapp.azure.com:9092,kafka-azure.norwayeast.cloudapp.azure.com:9091,kafka-azure.norwayeast.cloudapp.azure.com:9090\"\n",
    "influxURL = \"https://us-east-1-1.aws.cloud2.influxdata.com/\"\n",
    "influxToken = \"TmlX0yIaYqlwgv5zwkVyMfGaTzKTJP1AvylAuEahfytsgTraUygURyXcblk4jE_GGCOIgUTKJzKT2fk48XzJFA==\"\n",
    "#:::::::::::ESQUEMA::::::::::::\n",
    "schema_url = requests.get('{}/subjects/{}/versions/latest/schema'.format(\"http://kafka-azure.norwayeast.cloudapp.azure.com:8081\", \"criptomonedas\"))\n",
    "schema = schema_url.text\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaSparkConsumer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#:::::::::::KAFKA CONSUMER::::::::::::\n",
    "data = (spark \n",
    "  .readStream \n",
    "  .format(\"kafka\") \n",
    "  .option(\"kafka.bootstrap.servers\", kafka_brokers) \n",
    "  .option(\"subscribe\", \"criptomonedas\") \n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"kafka.group.id\", \"Coins\")\n",
    "  .option(\"kafka.client.id\", \"Databricks\")\n",
    "  .option(\"enable.auto.commit\", \"True\")\n",
    "  .load())\n",
    "\n",
    "# Conversion y creacion de un dataframe a partir del esquema recuperado\n",
    "# 1. Quitamos los bytes sobrantes del esquema-registro Avro\n",
    "# 2. Convertimos de avro a json\n",
    "# 3. Sacamos el nombre a partir de la columna 'key'\n",
    "# 4. Creamos una estructura tabular seleccionando el clave-valor de la columna value (el JSON)\n",
    "converted_data = (data\n",
    "    .withColumn('value', expr(\"substring(value,6)\"))\n",
    "    .withColumn('value', from_avro(col('value'), schema))\n",
    "    .withColumn('key', col('key').cast('string'))\n",
    "    .select(\"key\",\"value.bid\", \"value.bid_size\", \"value.ask\", \"value.ask_size\", \"value.daily_change\", \"value.daily_change_percentage\", \"value.precio_ultimo\", \"value.volume\", \"value.precio_maximo\", \"value.precio_minimo\", \"timestamp\")\n",
    "    )\n",
    "\n",
    "#display(converted_data)\n",
    "\n",
    "# Configuración de InfluxDB\n",
    "influxdb_client = InfluxDBClient(url=influxURL, token=influxToken, org=\"Viewnext\")\n",
    "bucket = \"criptodata\"\n",
    "write_api = influxdb_client.write_api(write_options=SYNCHRONOUS)\n",
    "\n",
    "# Función para consumir mensajes y escribir en InfluxDB\n",
    "def write_to_influx(df, batch_id):\n",
    "  write_api = influxdb_client.write_api(write_options=SYNCHRONOUS)\n",
    "  records = df.collect()\n",
    "  for mensaje in records:\n",
    "      registro = (\n",
    "        Point(\"criptomonedas\")\n",
    "        .tag(\"Moneda\", mensaje['key'])\n",
    "        .field(\"Precio\", mensaje['precio_ultimo'])\n",
    "        .time(mensaje['timestamp'], WritePrecision.NS)\n",
    "        )\n",
    "      write_api.write(bucket=bucket, org=\"Viewnext\", record=registro)\n",
    "\n",
    "# Escribir los datos en InfluxDB\n",
    "query_influx = converted_data.writeStream.foreachBatch(write_to_influx).start()\n",
    "display(query_influx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "520860cf-8e77-4cfe-a8a8-35d8d9e33c97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Función para consumir mensajes y escribir en InfluxDB\n",
    "def write_to_influx(df):\n",
    "    records = df.collect()\n",
    "    for mensaje in records:\n",
    "        registro = (\n",
    "            Point(\"criptomonedas\")\n",
    "            .tag(\"Moneda\", mensaje['key'])\n",
    "            .field(\"Precio\", mensaje['precio_ultimo'])\n",
    "            .time(mensaje['timestamp'], WritePrecision.NS)\n",
    "        )\n",
    "        write_api.write(bucket=bucket, org=\"Viewnext\", record=registro)\n",
    "\n",
    "# Escribir los datos en InfluxDB\n",
    "query_influx = data.writeStream.foreachBatch(write_to_influx).start()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks-Consumer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
